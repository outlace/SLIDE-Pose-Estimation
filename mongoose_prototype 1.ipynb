{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONGOOSE Prototype 1\n",
    "### Goal: Implement smart hash-table update scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintain two copies of the parameters $W$ (trainable params) and $V$ (lagged copy of V).\n",
    "Maintain a list $S$ of the nodes $w_i$ in $W$ where $||w_i - v_i|| > \\epsilon$ where $\\epsilon$ is some error threshold parameter (e.g. $v_i$ are stale). So $S$ tracks the number of node indices that are stale. \n",
    "\n",
    "Everytime we query the hash table, we also compute $S$. If $|S| > \\rho$ then we initiate an LSH table update of just those stale nodes, and also include nodes that are close to the decision-boundary $\\epsilon$ if there are many nodes close to the decision boundary, e.g. $S \\leftarrow w_i, if ||w_i - v_i|| > \\epsilon / 2 \\text{ and } i > \\rho$ where $\\rho$ is some parameter, e.g. if 5% of the nodes are near the decision boundary then do a full update of those too, because when there are a lot nodes close to the decision-boundary then they are likely to become stale soon anyway.\n",
    "\n",
    "So every LSH query we compute $S$ and then use that to decide whether to initiate a hashtable update of selected stale nodes. When we update the nodes in $S$, we also update the lagged copy of the parameters $V$ for those nodes so that their synchronized with the main parameters $W$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Major issue: Training is exploding in memory for unclear reasons. Somehow related to check_stale and updating hash tables. Another issues is that with multithreading it dies; possible due to corrupting the hash tables by simultaneously writing since more updates are happening more frequently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Flux\n",
    "using Zygote\n",
    "using MLDatasets\n",
    "using LSHFunctions\n",
    "using DataStructures\n",
    "using Plots;\n",
    "using Profile;\n",
    "using StatProfilerHTML;\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra;\n",
    "using Random;\n",
    "using Statistics\n",
    "#using Distributions;\n",
    "include(\"./Optim.jl\");\n",
    "using .Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "batch_size = 8\n",
    "hash_len = 5  #k,L= 7,10\n",
    "n_tables = 10;\n",
    "bin_size = 30\n",
    "sample_prop = 0.01 # 5%, proportion of nodes to sample from matrix\n",
    "update_thresh_eps = 0.05 # minimum difference between lagged vector and trained vector to consider stale, ∈ [0,0.1]\n",
    "update_thresh_num = 0.1; # percentage of nodes that are stale before triggering update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function onehotencode(y)\n",
    "    t = zeros(Float64,10)\n",
    "    t[y[]+1] = 1.0\n",
    "    return t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = MNIST.traindata();\n",
    "train_x = permutedims(train_x,(3,2,1))\n",
    "x_train = reshape(train_x,(size(train_x)[1],prod(size(train_x)[2:end])));\n",
    "y_train = [transpose(onehotencode(y)) for y in train_y]\n",
    "y_train = vcat(y_train...);\n",
    "x_train = convert(Array{Float64}, x_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = MNIST.testdata();\n",
    "test_x = permutedims(test_x,(3,2,1))\n",
    "x_test = reshape(test_x,(size(test_x)[1],prod(size(test_x)[2:end])));\n",
    "y_test = [transpose(onehotencode(y)) for y in test_y]\n",
    "y_test = vcat(y_test...);\n",
    "x_test = convert(Array{Float64}, x_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tid = rand(1:60000)\n",
    "#println(y_train[tid,:])\n",
    "#Gray.(reshape(x_train[tid,:],(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2norm(x) = √(dot(x,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average(vs,n) = [sum(@view vs[i:(i+n-1)])/n for i in 1:(length(vs)-(n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function scale(x)\n",
    "    n = length(x)\n",
    "    s = size(x)\n",
    "    r1 = Array{Float64}(undef,n)\n",
    "    x_ = reshape(x,n)\n",
    "    mn = minimum(x_)\n",
    "    r1 = (x_ .- mn)\n",
    "    mx = maximum(r1)\n",
    "    r2 = Array{Float64}(undef,n)\n",
    "    r2 = (r1 ./ mx)\n",
    "    reshape(r2,s)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1,0,5] |> scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function standardize(x)\n",
    "    μ = mean(x)\n",
    "    σ = std(x)\n",
    "    r = (x .- μ) ./ σ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function comb(n, len) \n",
    "    Iterators.product(fill(BitArray([0,1]), len)...) |> collect |> vec \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hash_codes_ = comb(1,hash_len);\n",
    "all_hash_codes = (all_hash_codes_[:] .|> BitVector);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FixLayer\n",
    "    theta::Matrix #original\n",
    "    bias::Matrix #original\n",
    "    theta_lag::Matrix\n",
    "    #S::Set{Int64} #stores stale node indices\n",
    "end\n",
    "\n",
    "mutable struct LSHLayer\n",
    "    theta::Matrix #original\n",
    "    bias::Matrix #original\n",
    "    theta_lag::Matrix #lagged copy for mongoose\n",
    "    hash_funs::Vector #Vector{SimHash}\n",
    "    hash_tables::Vector{Dict{BitVector,CircularBuffer{Integer}}}\n",
    "    #sampled::Vector\n",
    "    #S::Set{Int64} #stores stale node indices\n",
    "end\n",
    "\n",
    "mutable struct ViewLayer\n",
    "    thetaV::SubArray\n",
    "    biasV::SubArray\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The `Layer` type constructor\n",
    "function LSHLayer(in_dim::Integer,out_dim::Integer,k=6,L=6,bin_size=bin_size)\n",
    "    div_n = 10\n",
    "    theta = randn(in_dim,out_dim) / div_n\n",
    "    bias = randn(1,out_dim) / div_n\n",
    "    cols = size(theta)[2] #number of columns/nodes\n",
    "    #hsh = SignALSH(k,maxnorm=5)\n",
    "    hash_funs = [SimHash(k) for i in 1:L] #LSHFunction(cossim, k)\n",
    "    hash_tables = Vector{Dict{BitVector,CircularBuffer{Integer}}}()\n",
    "    for i in 1:L #create L hash tables\n",
    "        ht_l::Dict{BitVector,CircularBuffer{Integer}} = Dict{BitVector,CircularBuffer{Integer}}((x) => CircularBuffer{Integer}(bin_size) for x in all_hash_codes)\n",
    "        push!(hash_tables,ht_l)\n",
    "        for j in 1:cols\n",
    "            t = theta[:,j]# |> standardize #Flux.normalize(theta[:,j])\n",
    "            hash_lj = hash_funs[i](t) |> BitVector\n",
    "            #hash_lj = Tuple(hash_lj)\n",
    "            push!(hash_tables[i][hash_lj],j)\n",
    "        end\n",
    "    end\n",
    "    return LSHLayer(theta,bias,copy(theta),hash_funs,hash_tables), ViewLayer(view(theta,:,:),view(bias,:,:))\n",
    "end\n",
    "\n",
    "function FixLayer(in_dim::Integer,out_dim::Integer)\n",
    "    div_n = 10\n",
    "    theta = randn(in_dim,out_dim) / div_n\n",
    "    bias = randn(1,out_dim) / div_n\n",
    "    return FixLayer(theta,bias,copy(theta)), ViewLayer(view(theta,:,:),view(bias,:,:))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_nodes(query::Vector, layer::LSHLayer)\n",
    "    #`query` is the input vector for this layer\n",
    "    S = Set{Int64}([])\n",
    "    #num_ht = copy(L)#layers[1].hash_funs |> length\n",
    "    cols = size(layer.theta)[2]\n",
    "    maxN::Int64 = (sample_prop * cols) |> round\n",
    "    x = copy(query)\n",
    "    while length(S) < maxN\n",
    "        i = rand(1:n_tables) #get random hash table index\n",
    "        # compute hash of query using each hashfun\n",
    "        q_hash = layer.hash_funs[i](x) |> BitVector\n",
    "        q_hash = rand(0:1,hash_len)\n",
    "        matches = layer.hash_tables[i][q_hash]\n",
    "        union!(S,matches)\n",
    "        if isempty(matches)\n",
    "            r = rand(1:cols,bin_size)\n",
    "            union!(S,r)\n",
    "        end\n",
    "    end\n",
    "    out = collect(S)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=@time begin Threads.@threads for i in 1:100000\n",
    "    r = rand(1:60000)\n",
    "    #print(\" $r \")\n",
    "    sample_nodes(x_train[r,:],layers[1])\n",
    "end\n",
    "end=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check_stale"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Input: an LSH layer\n",
    "Output: vector of node indices\n",
    "\n",
    "This function will compare the trained parameters and the lagged copy of the parameters to find those nodes that\n",
    "are stale, i.e. where the lagged copy is significantly different from the main copy.\n",
    "\"\"\"\n",
    "function check_stale(stale::Set{Int64},layer::LSHLayer)\n",
    "    \n",
    "    rows, cols = size(layer.theta)\n",
    "    notS = setdiff(1:cols,stale)\n",
    "    maxN::Int64 = (0.1 * rows) |> round\n",
    "    rid = rand(1:rows,maxN)\n",
    "    theta = layer.theta[rid,:]\n",
    "    theta_lag = layer.theta_lag[rid,:]\n",
    "    td = theta .- theta_lag\n",
    "    ϵ = update_thresh_eps #0.001 * l2norm(theta) #update_thresh_eps\n",
    "    S::Set{Int64} = Set{Int64}([])\n",
    "    S₂::Set{Int64} = Set{Int64}([])\n",
    "    t = (ϵ * cols) |> round\n",
    "    close_bound = ( (0.9) * ϵ ) #close to decision boundary\n",
    "    for i in notS\n",
    "        a = td[:,i]\n",
    "        z = l2norm(a)\n",
    "        if z >= ϵ\n",
    "            push!(S, i)\n",
    "        elseif z >= close_bound #if there are a lot of points close to decision boundary\n",
    "            push!(S₂, i)\n",
    "        end\n",
    "    end\n",
    "    #println(length(S₂))\n",
    "    if length(S₂) > (1.5 * t)\n",
    "        S = (S ∪ S₂)\n",
    "    end\n",
    "    R = copy(stale ∪ S)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_ = Set{Int64}([])\n",
    "#S2 = check_stale(s_,layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function get_view(a::Matrix,r,c)\n",
    "    #r,c are arrays, if empty [] then return all elements\n",
    "    d1,d2 = size(a);\n",
    "    return view(a, isempty(r) ? (:) : r, isempty(c) ? (:) : c)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that runs an LSH layer\n",
    "function (m::LSHLayer)(X::Matrix,rows::Vector,V::ViewLayer)\n",
    "    Zygote.ignore() do\n",
    "        cols = sample_nodes(vec(X),m) #rand(1:size(m.theta)[2],90)\n",
    "        cols = sort(cols);\n",
    "        V.thetaV = get_view(m.theta,rows,cols)\n",
    "        V.biasV = get_view(m.bias,[],cols)\n",
    "    end\n",
    "    y = X * V.thetaV .+ V.biasV\n",
    "end\n",
    "#no activation function applied yet\n",
    "\n",
    "function (m::FixLayer)(X::Matrix,rows::Vector,V::ViewLayer)\n",
    "    Zygote.ignore() do\n",
    "        V.thetaV = get_view(m.theta,rows,[])\n",
    "        V.biasV = get_view(m.bias,[],[])\n",
    "    end\n",
    "    y = X * V.thetaV .+ V.biasV\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:red\">Initialize Layers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1,dim2,dim3 = 784, 1000, 10\n",
    "layer1, layer1view = LSHLayer(dim1,dim2, hash_len, n_tables) #k,L\n",
    "layer2, layer2view = FixLayer(dim2,dim3)\n",
    "layers = [layer1, layer2];\n",
    "layerviews = [layer1view, layer2view];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_view_indices(layerview)\n",
    "   return layerview.thetaV.indices[2] \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_view_indices(layerviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model (generic function with 1 method)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function model(X::Matrix,layers::Vector,layerviews::Vector)\n",
    "    #layer 1\n",
    "    X = Flux.normalise(X;dims=ndims(X), ϵ=1e-5) #\n",
    "    A1 = (layers[1](X,Vector{Integer}[],layerviews[1]))\n",
    "    A1 = NNlib.relu.(A1)\n",
    "    #layer 2\n",
    "    A2 = Flux.normalise(A1;dims=ndims(A1), ϵ=1e-5)\n",
    "    A2 = (layers[2](A2,get_view_indices(layerviews[1]),layerviews[2]))#layers[1].sampled\n",
    "    A2 = NNlib.softmax(A2,dims=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn(ŷ::Vector,y::Vector) = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "\n",
    "function lossfn2(x::Matrix,y::Vector,layers::Vector,layerviews::Vector)\n",
    "    ŷ = vec(model(x, layers, layerviews));\n",
    "    l = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_htables(S::Set{Int64},ms::Vector)\n",
    "    u = 0\n",
    "    for layer in ms\n",
    "        num_ht = n_tables #length(layer.hash_funs)\n",
    "        cols = size(layer.theta)[2]\n",
    "        ns = length(S)\n",
    "        t = (update_thresh_num * cols) |> round\n",
    "        if ns >= t\n",
    "            u += 1\n",
    "            while !isempty(S)\n",
    "                j = pop!(S)\n",
    "                for i in 1:num_ht #iterate tables\n",
    "                    z = layer.theta[:,j]# |> standardize\n",
    "                    hash_lj = layer.hash_funs[i](z) |> BitVector\n",
    "                    #hash_lj = Tuple(hash_lj)\n",
    "                    push!(layer.hash_tables[i][hash_lj],j)\n",
    "                    layer.theta_lag[:,j] = copy(layer.theta[:,j])\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return S,u\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn(ŷ::Vector,y::Vector) = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "\n",
    "function lossfn2(x::Matrix,y::Vector,layers::Vector,layerviews::Vector)\n",
    "    ŷ = vec(model(x, layers, layerviews));\n",
    "    l = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = [1,50,90,112,145,240,300,301,500,505,506,511];\n",
    "#g = Zygote.gradient(w -> lossfn(vec(model(randn(1,784),w,S)),[1.0,0,0,0,0,0,0,0,0,0]),layers)\n",
    "#g = Zygote.gradient((ypred,ytrue) -> lossfn(ypred,ytrue),(A2,batch_y))\n",
    "#println(g);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_params_view(layerviews::Vector)\n",
    "    #idx: [rows1,cols2,rows2,cols2]\n",
    "    p = []\n",
    "    #i = 0\n",
    "    for m in layerviews\n",
    "        #theta = get_view(m.theta,m.rows,m.cols)\n",
    "        #bias = get_view(m.bias,[],m.cols)\n",
    "        append!(p,[m.thetaV,m.biasV])\n",
    "        #i+=1\n",
    "    end\n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function convert_grads(gs)\n",
    "    g = []\n",
    "    for l in gs[1]\n",
    "        append!(g,[l[][:thetaV],l[][:biasV]])\n",
    "    end\n",
    "    g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(randn(1,784), layers, layerviews)\n",
    "#w/ scale 26.083 μs (94 allocations: 21.12 KiB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lossfn2(randn(1,784),y_train[1,:],layers,layerviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gen_layerviews(layers)\n",
    "    V = []\n",
    "    for l in layers\n",
    "        push!(V, ViewLayer(view(l.theta,:,:),view(l.bias,:,:)))\n",
    "    end\n",
    "    V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_batch(x_train,y_train,batch_size=1)\n",
    "    rid = rand(1:60000,batch_size)\n",
    "    x = float(copy(x_train[rid,:]))\n",
    "    x = reshape(x,(1,784))\n",
    "    yt = copy(y_train[rid,:])\n",
    "    yt = reshape(yt,(10))\n",
    "    return x,yt\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xt,yt = get_batch(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 3 methods)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(x_train,y_train,layers,epochs=200000,lr=0.5)\n",
    "    opt = SGDM(lr, 0.9, [])#ADAM(lr)\n",
    "    #ps = get_params_view(layerviews);\n",
    "    #opt = Optim.ADAM(ps, lr)\n",
    "    lossarr::Vector{Float64} = []\n",
    "    updates = 0\n",
    "    #lk = ReentrantLock()\n",
    "    S::Set{Int64} = Set([])\n",
    "    Threads.@threads for i in 1:epochs #Threads.@threads \n",
    "        xt,yt = get_batch(x_train,y_train)\n",
    "        layerviews = gen_layerviews(layers)\n",
    "        l, gs = withgradient((mv) -> lossfn2(xt,yt,layers,mv), layerviews)\n",
    "        gs = convert_grads(gs)\n",
    "        ps = get_params_view(layerviews);\n",
    "        step!(opt,ps,gs)\n",
    "        S = check_stale(copy(S),layers[1])\n",
    "        #print(\" $(length(S)) \")\n",
    "        #(i % 50 == 0) ? print(\" $(length(S)) \") : false\n",
    "        S, u = update_htables(S, layers[1:1])\n",
    "        updates += u\n",
    "        #=\n",
    "        lock(lk) do\n",
    "            #do something\n",
    "            push!(lossarr,l)\n",
    "        end\n",
    "        =#\n",
    "    end\n",
    "    println(\"Num hash table updates: $updates\")\n",
    "    return lossarr\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=@profilehtml for i = 1:100\n",
    "    l, gs = withgradient((mv) -> lossfn2(randn(1,784),randn(10),layers,mv), layerviews)\n",
    "end=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num hash table updates: 1017\n",
      " 16.615596 seconds (132.81 M allocations: 93.974 GiB, 24.71% gc time, 0.72% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Float64[]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@time train(x_train,y_train,layers,25000,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=@time begin\n",
    "    lossarr = train(x_train,y_train,layers,25000);\n",
    "end=#\n",
    "# ~141 seconds for 250k iter with 2-layers and dim2 = 1000 \n",
    "# 72 seconds for 25k iter\n",
    "# 32.9 seconds 25k no updates\n",
    "# 56 seconds 25k w/ updates 63 ht updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(moving_average(lossarr,5000)) #y top is > 3, x right is 2 x 10^5\n",
    "#plot(lossarr[1:2250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function test_acc(xs,ys)\n",
    "    ncorr = 0\n",
    "    ntot = size(ys)[1]\n",
    "    #lk = ReentrantLock()\n",
    "    for i in 1:ntot\n",
    "        #rid = rand(1:size(xs)[1])\n",
    "        x = float(xs[i,:])\n",
    "        x = reshape(x,(1,784))\n",
    "        yt = ys[i,:]\n",
    "        ŷ = vec(model(x, layers, layerviews));\n",
    "        if argmax(ŷ) == argmax(yt)\n",
    "            ncorr += 1\n",
    "        end\n",
    "    end\n",
    "    println(100 * (ncorr/ntot))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68.44\n"
     ]
    }
   ],
   "source": [
    "test_acc(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model = Chain(Dense(28^2, dim2, relu), Flux.normalise, Dense(dim2, 10), softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_model(x_train[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "#=\n",
    "function base_train(epochs=100)\n",
    "    baseopt = Flux.ADAM(0.0001); \n",
    "    for i in 1:epochs\n",
    "        rid = rand(1:60000)\n",
    "        x = float(x_train[rid,:])\n",
    "        #x = reshape(x,(1,784))\n",
    "        yt = y_train[rid,:]\n",
    "        ps = Flux.params(base_model)\n",
    "        gs = gradient(ps) do\n",
    "            ŷ = base_model(x)\n",
    "            lossfn(ŷ,yt)\n",
    "        end\n",
    "        Flux.update!(baseopt,ps,gs)\n",
    "    end\n",
    "end\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=@time begin\n",
    "    base_train(25000)\n",
    "end=#\n",
    "#25k iter: 111 seconds to 95% acc, max acc is 97.5 (if train to convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=function base_test_acc(xs,ys)\n",
    "    ncorr = 0\n",
    "    ntot = size(ys)[1]\n",
    "    for i in 1:ntot\n",
    "        #rid = rand(1:size(xs)[1])\n",
    "        x = float(xs[i,:])\n",
    "        #x = reshape(x,(1,784))\n",
    "        yt = ys[i,:]\n",
    "        ŷ = vec(base_model(x));\n",
    "        if argmax(ŷ) == argmax(yt)\n",
    "            ncorr += 1\n",
    "        end\n",
    "    end\n",
    "    println(100 * (ncorr/ntot))\n",
    "end=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_test_acc(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ecd5b0def40d11926eb769baa0f5c5b6e342746dd5b6c5b03c4754dea9d1700"
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
