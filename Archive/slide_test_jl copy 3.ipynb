{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Flux\n",
    "using Zygote\n",
    "using MLDatasets\n",
    "using LSHFunctions\n",
    "using DataStructures\n",
    "using Plots;\n",
    "using Profile;\n",
    "using StatProfilerHTML;\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra;\n",
    "using SparseArrays\n",
    "include(\"./Optim.jl\");\n",
    "using .Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "const batch_size = 16\n",
    "const k = 5  #k,L= 7,10\n",
    "const L = 10;\n",
    "#const sample_rate = 0.1 # 1%, proportion of nodes to sample from matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "onehotencode (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function onehotencode(y)\n",
    "    t = zeros(Float64,10)\n",
    "    t[y[]+1] = 1.0\n",
    "    return t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = MNIST.traindata();\n",
    "train_x = permutedims(train_x,(3,2,1))\n",
    "x_train = reshape(train_x,(size(train_x)[1],prod(size(train_x)[2:end])));\n",
    "y_train = [transpose(onehotencode(y)) for y in train_y]\n",
    "y_train = vcat(y_train...);\n",
    "x_train = convert(Array{Float64}, x_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = MNIST.testdata();\n",
    "test_x = permutedims(test_x,(3,2,1))\n",
    "x_test = reshape(test_x,(size(test_x)[1],prod(size(test_x)[2:end])));\n",
    "y_test = [transpose(onehotencode(y)) for y in test_y]\n",
    "y_test = vcat(y_test...);\n",
    "x_test = convert(Array{Float64}, x_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tid = rand(1:60000)\n",
    "#println(y_train[tid,:])\n",
    "#Gray.(reshape(x_train[tid,:],(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "moving_average (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moving_average(vs,n) = [sum(@view vs[i:(i+n-1)])/n for i in 1:(length(vs)-(n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comb (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function comb(n, len) \n",
    "    Iterators.product(fill(BitArray([0,1]), len)...) |> collect |> vec \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hash_codes = comb(1,k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct LSHLayer\n",
    "    theta::Matrix #original\n",
    "    bias::Matrix #original\n",
    "    thetaV::SubArray\n",
    "    biasV::SubArray\n",
    "    hash_funs::Vector{SimHash}\n",
    "    hash_tables::Vector{Dict{Tuple,CircularBuffer{Integer}}}\n",
    "    #rows::Vector\n",
    "    #cols::Vector\n",
    "    sampled::Vector\n",
    "end\n",
    "\n",
    "mutable struct FixLayer\n",
    "    theta::Matrix #original\n",
    "    bias::Matrix #original\n",
    "    thetaV::SubArray\n",
    "    biasV::SubArray\n",
    "    #rows::Vector\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixLayer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The `Layer` type constructor\n",
    "function LSHLayer(in_dim::Integer,out_dim::Integer,k=6,L=6,bin_size=10)\n",
    "    div_n = 10\n",
    "    theta = randn(in_dim,out_dim) / div_n\n",
    "    bias = randn(1,out_dim) / div_n\n",
    "    cols = size(theta)[2] #number of columns/nodes\n",
    "    hash_funs = [LSHFunction(cossim, k) for i in 1:L]\n",
    "    hash_tables = Vector{Dict{Tuple,CircularBuffer{Integer}}}()\n",
    "    for i in 1:L #create L hash tables\n",
    "        ht_l::Dict{Tuple,CircularBuffer{Integer}} = Dict{Tuple,CircularBuffer{Integer}}((x) => CircularBuffer{Integer}(bin_size) for x in all_hash_codes)\n",
    "        push!(hash_tables,ht_l)\n",
    "        for j in 1:cols\n",
    "            hash_lj = hash_funs[i](theta[:,j])\n",
    "            hash_lj = Tuple(hash_lj)\n",
    "            push!(hash_tables[i][hash_lj],j)\n",
    "        end\n",
    "    end\n",
    "    LSHLayer(theta,bias,view(theta,:,:),view(bias,:,:),hash_funs,hash_tables,[])\n",
    "end\n",
    "\n",
    "function FixLayer(in_dim::Integer,out_dim::Integer)\n",
    "    div_n = 10\n",
    "    theta = randn(in_dim,out_dim) / div_n\n",
    "    bias = randn(1,out_dim) / div_n\n",
    "    FixLayer(theta,bias,view(theta,:,:),view(bias,:,:))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_nodes (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sample_nodes(query::Vector, layer::LSHLayer)\n",
    "    #`query` is the input vector for this layer\n",
    "    S = Set{Int64}()\n",
    "    num_ht = layers[1].hash_funs |> length\n",
    "    for i in 1:num_ht\n",
    "        # compute hash of query using each hashfun\n",
    "        q_hash = layer.hash_funs[i](query) |> Tuple\n",
    "        matches = layer.hash_tables[i][q_hash]\n",
    "        union!(S,matches)\n",
    "    end\n",
    "    if isempty(S)\n",
    "        push!(S,rand(1:size(layer.theta)[2]))\n",
    "    end\n",
    "    return S |> collect\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.trainable(a::LSHLayer) = (a.thetaV,a.biasV)\n",
    "Flux.trainable(a::FixLayer) = (a.thetaV,a.biasV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_view (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_view(a::Matrix,r,c)\n",
    "    #r,c are arrays, if empty [] then return all elements\n",
    "    d1,d2 = size(a);\n",
    "    return view(a, isempty(r) ? (:) : r, isempty(c) ? (:) : c)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that runs an LSH layer\n",
    "function (m::LSHLayer)(X::Matrix,rows::Vector)\n",
    "    Zygote.ignore() do\n",
    "        cols = sample_nodes(vec(X),m) #rand(1:size(m.theta)[2],90)\n",
    "        cols = sort(cols);\n",
    "        m.thetaV = get_view(m.theta,rows,cols)\n",
    "        m.biasV = get_view(m.bias,[],cols)\n",
    "        m.sampled = cols\n",
    "    end\n",
    "    y = X * m.thetaV .+ m.biasV\n",
    "end\n",
    "#no activation function applied yet\n",
    "\n",
    "function (m::FixLayer)(X::Matrix,rows::Vector)\n",
    "    Zygote.ignore() do\n",
    "        m.thetaV = get_view(m.theta,rows,[])\n",
    "        m.biasV = get_view(m.bias,[],[])\n",
    "    end\n",
    "    y = X * m.thetaV .+ m.biasV\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim2, dim3 = 1000, 500\n",
    "layer1 = LSHLayer(784,dim2, k, 10) #k,L\n",
    "#layer1 = FixLayer(784,dim2)\n",
    "#layer2 = LSHLayer(dim2,dim3, k, L)\n",
    "layer2 = FixLayer(dim2,10)\n",
    "layers = [layer1, layer2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function model(X::Matrix,layers::Vector)\n",
    "    #layer 1\n",
    "    X = Flux.normalise(X;dims=ndims(X), ϵ=1e-5)\n",
    "    A1 = layers[1](X,Vector{Integer}[])\n",
    "    A1 = NNlib.relu.(A1)\n",
    "    #layer 2\n",
    "    #rows1 = isempty(layers[1].cols) ? sample_nodes(vec(X),layers[1]) : layers[2].cols\n",
    "    #=\n",
    "    A1 = Flux.normalise(A1;dims=ndims(A1), ϵ=1e-5)\n",
    "    A2 = layers[2](A1,layers[1].sampled)\n",
    "    #println(size(A2),length(n2))\n",
    "    A2 = NNlib.sigmoid.(A2)\n",
    "    =#\n",
    "    A2 = Flux.normalise(A1;dims=ndims(A1), ϵ=1e-5)\n",
    "    A2 = layers[2](A2,layers[1].sampled)#layers[1].sampled\n",
    "    A2 = NNlib.softmax(A2,dims=2)\n",
    "end\n",
    "\n",
    "#function model(X::Matrix,layers::Vector{Layer})\n",
    "#    model_(X, layers)[1]\n",
    "#end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update_htables (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function update_htables(layer::LSHLayer)\n",
    "    num_ht = length(layer.hash_funs)\n",
    "    cols = size(layer.theta)[2]\n",
    "    for i in 1:num_ht #iterate tables\n",
    "        for j in 1:cols\n",
    "            hash_lj = layer.hash_funs[i](layer.theta[:,j])\n",
    "            hash_lj = Tuple(hash_lj)\n",
    "            push!(layer.hash_tables[i][hash_lj],j)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lossfn2 (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossfn(ŷ::Vector,y::Vector) = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "\n",
    "function lossfn2(x::Matrix,layers::Vector,y::Vector)\n",
    "    ŷ = vec(model(x, layers));\n",
    "    l = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = [1,50,90,112,145,240,300,301,500,505,506,511];\n",
    "#g = Zygote.gradient(w -> lossfn(vec(model(randn(1,784),w,S)),[1.0,0,0,0,0,0,0,0,0,0]),layers)\n",
    "#g = Zygote.gradient((ypred,ytrue) -> lossfn(ypred,ytrue),(A2,batch_y))\n",
    "#println(g);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_params_view (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_params_view(layers)\n",
    "    #idx: [rows1,cols2,rows2,cols2]\n",
    "    p = []\n",
    "    #i = 0\n",
    "    for m in layers\n",
    "        #theta = get_view(m.theta,m.rows,m.cols)\n",
    "        #bias = get_view(m.bias,[],m.cols)\n",
    "        append!(p,[m.thetaV,m.biasV])\n",
    "        #i+=1\n",
    "    end\n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convert_grads2 (generic function with 1 method)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function convert_grads(gs)\n",
    "    g = []\n",
    "    for l in gs[1]\n",
    "        append!(g,[l[][:thetaV],l[][:biasV]])\n",
    "    end\n",
    "    g\n",
    "end\n",
    "\n",
    "function convert_grads2(grads, layers)\n",
    "    gs = []\n",
    "    iz = []\n",
    "    for (l,g) in zip(layers,grads[1])\n",
    "        append!(gs,[g[][:thetaV],g[][:biasV]])\n",
    "        isdefined(l,:sampled) ? push!(iz,l.sampled) : push!(iz,[])\n",
    "    end\n",
    "    collect(zip(gs,iz))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_params_view(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g[1][1][][:theta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Matrix{Float64}:\n",
       " 0.065631  0.0547397  0.00787681  0.100399  …  0.241617  0.155482  0.127214"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(randn(1,784), layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lossfn2(randn(1,784),layers,[1,0,0,0,0,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(x_train,y_train,layers,epochs=200000)\n",
    "    N₀ = 50\n",
    "    λ₀ = 0.6\n",
    "    lr = 0.001\n",
    "    update_freq = round.([N₀ * exp(λ₀*x) for x in LinRange(0,5,epochs)])\n",
    "    opt = SGDM(lr, 0.9, [])#ADAM(lr)\n",
    "    lossarr = []\n",
    "    updates = 0\n",
    "    Threads.@threads for i in 1:epochs\n",
    "        rid = rand(1:60000)\n",
    "        x = float(x_train[rid,:])\n",
    "        x = reshape(x,(1,784))\n",
    "        yt = y_train[rid,:]\n",
    "        l, gs = withgradient((ms) -> lossfn2(x,ms,yt), layers)\n",
    "        push!(lossarr,l)\n",
    "        gs = convert_grads(gs)\n",
    "        ps = get_params_view(layers);\n",
    "        step!(opt,ps,gs)\n",
    "        #=\n",
    "        if i % update_freq[i] == 0\n",
    "            update_htables(layers[1])\n",
    "            updates += 1 \n",
    "        end\n",
    "        =#\n",
    "    end\n",
    "    println(\"Num hash table updates: $updates\")\n",
    "    return lossarr\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = SGDM(0.001, 0.9, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=gs_ = gradient((ms) -> lossfn2(randn(1,784),ms,y_train[1,:]), layers)\n",
    "gs = convert_grads(gs_)\n",
    "ps = get_params_view(layers)\n",
    "step3!(opt,ps,gs)=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "for (a,b) in zip(ps,gs)\n",
    "    println(size(a),size(b))\n",
    "end\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step!(opt,ps,gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_grads(gs)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=\n",
    "ps_ = [layers[1].thetaV,layers[1].biasV,layers[2].thetaV,layers[2].biasV]#get_params_view(layers);\n",
    "ps = Flux.Params(ps_);\n",
    "gs = Flux.gradient(ps) do\n",
    "    #lossfn2(reshape(x_train[1,:],(1,784)),layers,y_train[1,:])\n",
    "    model(reshape(x_train[1,:],(1,784)),layers) |> sum\n",
    "end\n",
    "=#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers[1].thetaV .-= gs[1][1][][:thetaV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(x_train,y_train,layers,50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@profilehtml train(x_train,y_train,layers,50000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "TaskFailedException\n\n\u001b[91m    nested task error: \u001b[39mDimensionMismatch(\"arrays could not be broadcast to a common size; got a dimension with lengths 90 and 88\")\n    Stacktrace:\n      [1] \u001b[0m\u001b[1m_bcs1\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:501\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [2] \u001b[0m\u001b[1m_bcs\u001b[22m\u001b[90m (repeats 2 times)\u001b[39m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:495\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [3] \u001b[0m\u001b[1mbroadcast_shape\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:489\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [4] \u001b[0m\u001b[1mcombine_axes\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:484\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [5] \u001b[0m\u001b[1minstantiate\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:266\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [6] \u001b[0m\u001b[1mmaterialize\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mbc\u001b[39m::\u001b[0mBase.Broadcast.Broadcasted\u001b[90m{Base.Broadcast.DefaultArrayStyle{2}, Nothing, typeof(+), Tuple{Matrix{Float64}, Matrix{Float64}}}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90mBase.Broadcast\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:883\u001b[0m\n      [7] \u001b[0m\u001b[1mstep!\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mopt\u001b[39m::\u001b[0mSGDM, \u001b[90mtheta\u001b[39m::\u001b[0mVector\u001b[90m{Any}\u001b[39m, \u001b[90mgrad\u001b[39m::\u001b[0mVector\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[35mMain.Optim\u001b[39m \u001b[90m~/Dropbox/My Mac (MacBook-Air.local)/Desktop/SLIDE Pose Estimation/\u001b[39m\u001b[90;4mOptim.jl:40\u001b[0m\n      [8] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[43]:18\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [9] \u001b[0m\u001b[1m(::var\"#3#threadsfor_fun#23\"{Matrix{Float64}, Matrix{Float64}, Vector{Any}, Vector{Any}, SGDM, UnitRange{Int64}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90monethread\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mthreadingconstructs.jl:81\u001b[0m\n     [10] \u001b[0m\u001b[1m(::var\"#3#threadsfor_fun#23\"{Matrix{Float64}, Matrix{Float64}, Vector{Any}, Vector{Any}, SGDM, UnitRange{Int64}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mthreadingconstructs.jl:48\u001b[0m",
     "output_type": "error",
     "traceback": [
      "TaskFailedException\n\n\u001b[91m    nested task error: \u001b[39mDimensionMismatch(\"arrays could not be broadcast to a common size; got a dimension with lengths 90 and 88\")\n    Stacktrace:\n      [1] \u001b[0m\u001b[1m_bcs1\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:501\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [2] \u001b[0m\u001b[1m_bcs\u001b[22m\u001b[90m (repeats 2 times)\u001b[39m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:495\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [3] \u001b[0m\u001b[1mbroadcast_shape\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:489\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [4] \u001b[0m\u001b[1mcombine_axes\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:484\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [5] \u001b[0m\u001b[1minstantiate\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:266\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [6] \u001b[0m\u001b[1mmaterialize\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mbc\u001b[39m::\u001b[0mBase.Broadcast.Broadcasted\u001b[90m{Base.Broadcast.DefaultArrayStyle{2}, Nothing, typeof(+), Tuple{Matrix{Float64}, Matrix{Float64}}}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90mBase.Broadcast\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mbroadcast.jl:883\u001b[0m\n      [7] \u001b[0m\u001b[1mstep!\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90mopt\u001b[39m::\u001b[0mSGDM, \u001b[90mtheta\u001b[39m::\u001b[0mVector\u001b[90m{Any}\u001b[39m, \u001b[90mgrad\u001b[39m::\u001b[0mVector\u001b[90m{Any}\u001b[39m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[35mMain.Optim\u001b[39m \u001b[90m~/Dropbox/My Mac (MacBook-Air.local)/Desktop/SLIDE Pose Estimation/\u001b[39m\u001b[90;4mOptim.jl:40\u001b[0m\n      [8] \u001b[0m\u001b[1mmacro expansion\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[90m./\u001b[39m\u001b[90;4mIn[43]:18\u001b[0m\u001b[90m [inlined]\u001b[39m\n      [9] \u001b[0m\u001b[1m(::var\"#3#threadsfor_fun#23\"{Matrix{Float64}, Matrix{Float64}, Vector{Any}, Vector{Any}, SGDM, UnitRange{Int64}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[90monethread\u001b[39m::\u001b[0mBool\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mthreadingconstructs.jl:81\u001b[0m\n     [10] \u001b[0m\u001b[1m(::var\"#3#threadsfor_fun#23\"{Matrix{Float64}, Matrix{Float64}, Vector{Any}, Vector{Any}, SGDM, UnitRange{Int64}})\u001b[22m\u001b[0m\u001b[1m(\u001b[22m\u001b[0m\u001b[1m)\u001b[22m\n    \u001b[90m    @ \u001b[39m\u001b[36mMain\u001b[39m \u001b[90m./\u001b[39m\u001b[90;4mthreadingconstructs.jl:48\u001b[0m",
      "",
      "Stacktrace:",
      " [1] wait",
      "   @ ./task.jl:317 [inlined]",
      " [2] threading_run(func::Function)",
      "   @ Base.Threads ./threadingconstructs.jl:34",
      " [3] macro expansion",
      "   @ ./threadingconstructs.jl:93 [inlined]",
      " [4] train(x_train::Matrix{Float64}, y_train::Matrix{Float64}, layers::Vector{Any}, epochs::Int64)",
      "   @ Main ./In[43]:9",
      " [5] macro expansion",
      "   @ ./In[45]:3 [inlined]",
      " [6] top-level scope",
      "   @ ./timing.jl:210 [inlined]",
      " [7] top-level scope",
      "   @ ./In[45]:0",
      " [8] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [9] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "#@profilehtml train(x_train,y_train,1000)\n",
    "@time begin\n",
    "    lossarr = train(x_train,y_train,layers,25000);\n",
    "end;\n",
    "# naive 50k hash updates\n",
    "# exponential decay: 1592 hash updates\n",
    "#192 seconds LSH\n",
    "#       fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: lossarr not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: lossarr not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[44]:1",
      " [2] eval",
      "   @ ./boot.jl:360 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1094"
     ]
    }
   ],
   "source": [
    "plot(moving_average(lossarr,1000)) #y top is > 3, x right is 2 x 10^5\n",
    "#plot(lossarr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_acc (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_acc(xs,ys)\n",
    "    ncorr = 0\n",
    "    ntot = size(ys)[1]\n",
    "    for i in 1:ntot\n",
    "        #rid = rand(1:size(xs)[1])\n",
    "        x = float(xs[i,:])\n",
    "        x = reshape(x,(1,784))\n",
    "        yt = ys[i,:]\n",
    "        ŷ = vec(model(x, layers));\n",
    "        if argmax(ŷ) == argmax(yt)\n",
    "            ncorr += 1\n",
    "        end\n",
    "    end\n",
    "    println(100 * (ncorr/ntot))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.08\n"
     ]
    }
   ],
   "source": [
    "test_acc(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#63.42 -50k, 1000dim, without hash table updates\n",
    "#63.83, with hash table updates\n",
    "#81.56 with %50 hash updates\n",
    "#80.21 without hash updates\n",
    "#80.16 with exp decy hash updates\n",
    "#81.3  with %25 updates\n",
    "#72.78% without hashing (random columns)\n",
    "#80.76 with hashing and sorted columns\n",
    "#84.95 div_n = 5\n",
    "#90.35 with k,L=5,7 div_n=5\n",
    "#90.66 k,L = 5,10\n",
    "#90.36     k,L = 5,20 div_n=5\n",
    "#93.55 k,L=5,10 with SGDM\n",
    "#97.06 with 2 fix layer and SGDM, but 1091.35 seconds run-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "function get_params_view(layers,idx)\n",
    "    #idx: [rows1,cols2,rows2,cols2]\n",
    "    p = []\n",
    "    i = 0\n",
    "    for layer in layers\n",
    "        theta = get_view(layer.theta,idx[2*i+1],idx[2*i+2])\n",
    "        bias = get_view(layer.bias,[],idx[2*i+2])\n",
    "        append!(p,[theta,bias])\n",
    "        i+=1\n",
    "    end\n",
    "    return p\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "function (m::Layer)(X::Matrix,rows,cols)\n",
    "    e1 = isempty(cols)\n",
    "    e2 = isempty(rows)\n",
    "    if e1 & e2 # neither rows nor cols given\n",
    "        y = X * m.theta .+ m.bias\n",
    "    elseif e2 #cols given alone\n",
    "        y = X * (@view m.theta[:,cols]) .+ (@view m.bias[:,cols]);\n",
    "    elseif e1 #rows given alone\n",
    "        y = X * (@view m.theta[rows,:]) .+ m.bias;\n",
    "    else # rows and cols given\n",
    "        y = X * (@view m.theta[rows,cols]) .+ (@view m.bias[:,cols]);\n",
    "    end\n",
    "    return y\n",
    "end\n",
    "```\n",
    "\n",
    "```julia\n",
    "function rescale(x_)\n",
    "    x = copy(x_)\n",
    "    mn = abs(minimum(x))\n",
    "    x = x .+ mn\n",
    "    mx = maximum(x)\n",
    "    x = x ./ mx\n",
    "    x\n",
    "endfunction rescale(x_)\n",
    "    x = copy(x_)\n",
    "    mn = abs(minimum(x))\n",
    "    x = x .+ mn\n",
    "    mx = maximum(x)\n",
    "    x = x ./ mx\n",
    "    x\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ecd5b0def40d11926eb769baa0f5c5b6e342746dd5b6c5b03c4754dea9d1700"
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
