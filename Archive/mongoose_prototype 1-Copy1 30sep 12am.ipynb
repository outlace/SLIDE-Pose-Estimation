{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONGOOSE Prototype 1\n",
    "### Goal: Implement smart hash-table update scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintain two copies of the parameters $W$ (trainable params) and $V$ (lagged copy of V).\n",
    "Maintain a list $S$ of the nodes $w_i$ in $W$ where $||w_i - v_i|| > \\epsilon$ where $\\epsilon$ is some error threshold parameter (e.g. $v_i$ are stale). So $S$ tracks the number of node indices that are stale. \n",
    "\n",
    "Everytime we query the hash table, we also compute $S$. If $|S| > \\rho$ then we initiate an LSH table update of just those stale nodes, and also include nodes that are close to the decision-boundary $\\epsilon$ if there are many nodes close to the decision boundary, e.g. $S \\leftarrow w_i, if ||w_i - v_i|| > \\epsilon / 2 \\text{ and } i > \\rho$ where $\\rho$ is some parameter, e.g. if 5% of the nodes are near the decision boundary then do a full update of those too, because when there are a lot nodes close to the decision-boundary then they are likely to become stale soon anyway.\n",
    "\n",
    "So every LSH query we compute $S$ and then use that to decide whether to initiate a hashtable update of selected stale nodes. When we update the nodes in $S$, we also update the lagged copy of the parameters $V$ for those nodes so that their synchronized with the main parameters $W$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Revise\n",
    "using Flux\n",
    "using Zygote\n",
    "using MLDatasets\n",
    "using LSHFunctions\n",
    "using DataStructures\n",
    "using Plots;\n",
    "using Profile;\n",
    "using StatProfilerHTML;\n",
    "using BenchmarkTools\n",
    "using LinearAlgebra;\n",
    "using SparseArrays\n",
    "using Random;\n",
    "include(\"./Optim.jl\");\n",
    "using .Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "batch_size = 8\n",
    "k = 5  #k,L= 7,10\n",
    "L = 7;\n",
    "bin_size = 20\n",
    "sample_prop = 0.05 # 5%, proportion of nodes to sample from matrix\n",
    "update_thresh_eps = 0.005 # minimum difference between lagged vector and trained vector to consider stale, ∈ [0,0.1]\n",
    "update_thresh_num = 0.05; # percentage of nodes that are stale before triggering update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function onehotencode(y)\n",
    "    t = zeros(Float64,10)\n",
    "    t[y[]+1] = 1.0\n",
    "    return t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = MNIST.traindata();\n",
    "train_x = permutedims(train_x,(3,2,1))\n",
    "x_train = reshape(train_x,(size(train_x)[1],prod(size(train_x)[2:end])));\n",
    "y_train = [transpose(onehotencode(y)) for y in train_y]\n",
    "y_train = vcat(y_train...);\n",
    "x_train = convert(Array{Float64}, x_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x, test_y = MNIST.testdata();\n",
    "test_x = permutedims(test_x,(3,2,1))\n",
    "x_test = reshape(test_x,(size(test_x)[1],prod(size(test_x)[2:end])));\n",
    "y_test = [transpose(onehotencode(y)) for y in test_y]\n",
    "y_test = vcat(y_test...);\n",
    "x_test = convert(Array{Float64}, x_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tid = rand(1:60000)\n",
    "#println(y_train[tid,:])\n",
    "#Gray.(reshape(x_train[tid,:],(28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_average(vs,n) = [sum(@view vs[i:(i+n-1)])/n for i in 1:(length(vs)-(n-1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function comb(n, len) \n",
    "    Iterators.product(fill(BitArray([0,1]), len)...) |> collect |> vec \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hash_codes = comb(1,k);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct FixLayer\n",
    "    theta::Matrix #original\n",
    "    bias::Matrix #original\n",
    "    theta_lag::Matrix\n",
    "    #S::Set{Int64} #stores stale node indices\n",
    "end\n",
    "\n",
    "mutable struct LSHLayer\n",
    "    theta::Matrix #original\n",
    "    bias::Matrix #original\n",
    "    theta_lag::Matrix #lagged copy for mongoose\n",
    "    hash_funs::Vector #Vector{SimHash}\n",
    "    hash_tables::Vector{Dict{Tuple,CircularBuffer{Integer}}}\n",
    "    #sampled::Vector\n",
    "    #S::Set{Int64} #stores stale node indices\n",
    "end\n",
    "\n",
    "mutable struct ViewLayer\n",
    "    thetaV::SubArray\n",
    "    biasV::SubArray\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The `Layer` type constructor\n",
    "function LSHLayer(in_dim::Integer,out_dim::Integer,k=6,L=6,bin_size=bin_size)\n",
    "    div_n = 10\n",
    "    theta = randn(in_dim,out_dim) / div_n\n",
    "    bias = randn(1,out_dim) / div_n\n",
    "    cols = size(theta)[2] #number of columns/nodes\n",
    "    #hsh = SignALSH(k,maxnorm=5)\n",
    "    hash_funs = [SimHash(k) for i in 1:L] #LSHFunction(cossim, k)\n",
    "    hash_tables = Vector{Dict{Tuple,CircularBuffer{Integer}}}()\n",
    "    for i in 1:L #create L hash tables\n",
    "        ht_l::Dict{Tuple,CircularBuffer{Integer}} = Dict{Tuple,CircularBuffer{Integer}}((x) => CircularBuffer{Integer}(bin_size) for x in all_hash_codes)\n",
    "        push!(hash_tables,ht_l)\n",
    "        for j in 1:cols\n",
    "            t = Flux.normalize(theta[:,j])\n",
    "            hash_lj = hash_funs[i](t)\n",
    "            hash_lj = Tuple(hash_lj)\n",
    "            push!(hash_tables[i][hash_lj],j)\n",
    "        end\n",
    "    end\n",
    "    return LSHLayer(theta,bias,copy(theta),hash_funs,hash_tables), ViewLayer(view(theta,:,:),view(bias,:,:))\n",
    "end\n",
    "\n",
    "function FixLayer(in_dim::Integer,out_dim::Integer)\n",
    "    div_n = 10\n",
    "    theta = randn(in_dim,out_dim) / div_n\n",
    "    bias = randn(1,out_dim) / div_n\n",
    "    return FixLayer(theta,bias,copy(theta)), ViewLayer(view(theta,:,:),view(bias,:,:))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sample_nodes(query::Vector, layer::LSHLayer)\n",
    "    #`query` is the input vector for this layer\n",
    "    S = Set{Int64}()\n",
    "    num_ht = layers[1].hash_funs |> length\n",
    "    cols = size(layer.theta)[2]\n",
    "    maxN::Int64 = (sample_prop * cols) |> round\n",
    "    while length(S) < maxN\n",
    "        i = rand(1:num_ht) #get random hash table index\n",
    "        # compute hash of query using each hashfun\n",
    "        q_hash = layer.hash_funs[i](query) |> Tuple \n",
    "        matches = layer.hash_tables[i][q_hash]\n",
    "        union!(S,matches)\n",
    "        if isempty(S)\n",
    "            push!(S,rand(1:cols))\n",
    "        end\n",
    "    end\n",
    "    return S |> collect\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: an LSH layer\n",
    "Output: vector of node indices\n",
    "\n",
    "This function will compare the trained parameters and the lagged copy of the parameters to find those nodes that\n",
    "are stale, i.e. where the lagged copy is significantly different from the main copy.\n",
    "\"\"\"\n",
    "function check_stale(layer)\n",
    "    ϵ = (update_thresh_eps / 2.0)\n",
    "    rows, cols = size(layer.theta)\n",
    "    notS = setdiff(1:cols,layer.S)\n",
    "    maxN::Int64 = (0.2 * rows) |> round\n",
    "    rid = rand(1:rows,maxN)\n",
    "    td = layer.theta[rid,:] .- layer.theta_lag[rid,:]\n",
    "    S::Set{Int64} = Set{Int64}([])\n",
    "    S₂::Set{Int64} = Set{Int64}([])\n",
    "    t = (update_thresh_num * cols) |> round\n",
    "    close_bound = ( (0.9) * ϵ )\n",
    "    for i in notS\n",
    "        a = td[:,i]\n",
    "        z = sqrt(dot(a,a))\n",
    "        if z >= ϵ\n",
    "            push!(S, i)\n",
    "        elseif z >= close_bound #if there are a lot of points close to decision boundary\n",
    "            push!(S₂, i)\n",
    "        end\n",
    "    end\n",
    "    #println(length(S₂))\n",
    "    if length(S₂) > (1.5 * t)\n",
    "        #S = (S ∪ S₂)\n",
    "    end\n",
    "    layer.S = (layer.S ∪ S)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_stale(layer1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layer1.theta_lag[:,rand(1:1000,90)] .+= randn(784)/100.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_stale(layer1)\n",
    "#parallel: 8 ms, array comprehension: 34 ms, for-loop: 35 ms, notS: 35ms\n",
    "#396 us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flux.trainable(a::LSHLayer) = (a.thetaV,a.biasV)\n",
    "#Flux.trainable(a::FixLayer) = (a.thetaV,a.biasV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function get_view(a::Matrix,r,c)\n",
    "    #r,c are arrays, if empty [] then return all elements\n",
    "    d1,d2 = size(a);\n",
    "    return view(a, isempty(r) ? (:) : r, isempty(c) ? (:) : c)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that runs an LSH layer\n",
    "function (m::LSHLayer)(X::Matrix,rows::Vector,V::ViewLayer)\n",
    "    Zygote.ignore() do\n",
    "        cols = sample_nodes(vec(X),m) #rand(1:size(m.theta)[2],90)\n",
    "        cols = sort(cols);\n",
    "        V.thetaV = get_view(m.theta,rows,cols)\n",
    "        V.biasV = get_view(m.bias,[],cols)\n",
    "        #m.sampled = cols\n",
    "    end\n",
    "    y = X * V.thetaV .+ V.biasV\n",
    "end\n",
    "#no activation function applied yet\n",
    "\n",
    "function (m::FixLayer)(X::Matrix,rows::Vector,V::ViewLayer)\n",
    "    Zygote.ignore() do\n",
    "        V.thetaV = get_view(m.theta,rows,[])\n",
    "        V.biasV = get_view(m.bias,[],[])\n",
    "    end\n",
    "    y = X * V.thetaV .+ V.biasV\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1,dim2,dim3 = 784, 1000, 10\n",
    "layer1, layer1view = LSHLayer(dim1,dim2, k, L) #k,L\n",
    "layer2, layer2view = FixLayer(dim2,dim3)\n",
    "layers = [layer1, layer2];\n",
    "layerviews = [layer1view, layer2view];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_view_indices(layerview)\n",
    "   return layerview.thetaV.indices[2] \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_view_indices(layerviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function model(X::Matrix,layers::Vector,layerviews::Vector)\n",
    "    #layer 1\n",
    "    X = Flux.normalise(X;dims=ndims(X), ϵ=1e-5)\n",
    "    A1 = layers[1](X,Vector{Integer}[],layerviews[1])\n",
    "    A1 = NNlib.relu.(A1)\n",
    "    #layer 2\n",
    "    A2 = Flux.normalise(A1;dims=ndims(A1), ϵ=1e-5)\n",
    "    A2 = layers[2](A2,get_view_indices(layerviews[1]),layerviews[2])#layers[1].sampled\n",
    "    A2 = NNlib.softmax(A2,dims=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn(ŷ::Vector,y::Vector) = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "\n",
    "function lossfn2(x::Matrix,y::Vector,layers::Vector,layerviews::Vector)\n",
    "    ŷ = vec(model(x, layers, layerviews));\n",
    "    l = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function update_htables(ms::Vector)\n",
    "    u = 0\n",
    "    for layer in ms\n",
    "        num_ht = length(layer.hash_funs)\n",
    "        cols = size(layer.theta)[2]\n",
    "        ns = length(layer.S)\n",
    "        t = (update_thresh_num * cols) |> round\n",
    "        if ns >= t\n",
    "            u += 1\n",
    "            while !isempty(layer.S)\n",
    "                j = pop!(layer.S)\n",
    "                for i in 1:num_ht #iterate tables\n",
    "                    hash_lj = layer.hash_funs[i](layer.theta[:,j])\n",
    "                    hash_lj = Tuple(hash_lj)\n",
    "                    push!(layer.hash_tables[i][hash_lj],j)\n",
    "                    layer.theta_lag[:,j] = copy(layer.theta[:,j])\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    return u\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossfn(ŷ::Vector,y::Vector) = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "\n",
    "function lossfn2(x::Matrix,y::Vector,layers::Vector,layerviews::Vector)\n",
    "    ŷ = vec(model(x, layers, layerviews));\n",
    "    l = -1.0 * LinearAlgebra.dot(log.(ŷ),y)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S = [1,50,90,112,145,240,300,301,500,505,506,511];\n",
    "#g = Zygote.gradient(w -> lossfn(vec(model(randn(1,784),w,S)),[1.0,0,0,0,0,0,0,0,0,0]),layers)\n",
    "#g = Zygote.gradient((ypred,ytrue) -> lossfn(ypred,ytrue),(A2,batch_y))\n",
    "#println(g);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function get_params_view(layerviews::Vector)\n",
    "    #idx: [rows1,cols2,rows2,cols2]\n",
    "    p = []\n",
    "    #i = 0\n",
    "    for m in layerviews\n",
    "        #theta = get_view(m.theta,m.rows,m.cols)\n",
    "        #bias = get_view(m.bias,[],m.cols)\n",
    "        append!(p,[m.thetaV,m.biasV])\n",
    "        #i+=1\n",
    "    end\n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function convert_grads(gs)\n",
    "    g = []\n",
    "    for l in gs[1]\n",
    "        append!(g,[l[][:thetaV],l[][:biasV]])\n",
    "    end\n",
    "    g\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(randn(1,784), layers, layerviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lossfn2(randn(1,784),y_train[1,:],layers,layerviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gen_layerviews(layers)\n",
    "    V = []\n",
    "    for l in layers\n",
    "        push!(V, ViewLayer(view(l.theta,:,:),view(l.bias,:,:)))\n",
    "    end\n",
    "    V\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function train(x_train,y_train,layers,epochs=200000)\n",
    "    N₀ = 50\n",
    "    λ₀ = 0.6\n",
    "    lr = 0.001\n",
    "    opt = SGDM(lr, 0.9, [])#ADAM(lr)\n",
    "    #ps = get_params_view(layerviews);\n",
    "    #opt = Optim.ADAM(ps, lr)\n",
    "    lossarr = []\n",
    "    updates = 0\n",
    "    lk = ReentrantLock()\n",
    "    Threads.@threads for i in 1:epochs #Threads.@threads\n",
    "        rid = rand(1:60000)\n",
    "        x = float(x_train[rid,:])\n",
    "        x = reshape(x,(1,784))\n",
    "        yt = y_train[rid,:]\n",
    "        layerviews = gen_layerviews(layers)\n",
    "        l, gs = withgradient((mv) -> lossfn2(x,yt,layers,mv), layerviews)\n",
    "        gs = convert_grads(gs)\n",
    "        ps = get_params_view(layerviews);\n",
    "        step!(opt,ps,gs)\n",
    "        S = check_stale(S, layers[1])\n",
    "        #print(\" $(length(layers[1].S)) \")\n",
    "        #(i % 50 == 0) ? print(\" $(length(S)) \") : false\n",
    "        S, updates += update_htables(S, layers[1:1])\n",
    "        #=\n",
    "        lock(lk) do\n",
    "            #do something\n",
    "            push!(lossarr,l)\n",
    "        end\n",
    "        =#\n",
    "    end\n",
    "    println(\"Num hash table updates: $updates\")\n",
    "    return lossarr\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = SGDM(0.001, 0.9, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time train(x_train,y_train,layers,500)\n",
    "# 2.4 s for 1k iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time begin\n",
    "    lossarr = train(x_train,y_train,layers,25000);\n",
    "end;\n",
    "# ~141 seconds for 250k iter with 2-layers and dim2 = 1000 \n",
    "# 72 seconds for 25k iter\n",
    "# 32.9 seconds 25k no updates\n",
    "# 56 seconds 25k w/ updates 63 ht updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(moving_average(lossarr,5000)) #y top is > 3, x right is 2 x 10^5\n",
    "#plot(lossarr[1:2250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function test_acc(xs,ys)\n",
    "    ncorr = 0\n",
    "    ntot = size(ys)[1]\n",
    "    #lk = ReentrantLock()\n",
    "    for i in 1:ntot\n",
    "        #rid = rand(1:size(xs)[1])\n",
    "        x = float(xs[i,:])\n",
    "        x = reshape(x,(1,784))\n",
    "        yt = ys[i,:]\n",
    "        ŷ = vec(model(x, layers, layerviews));\n",
    "        if argmax(ŷ) == argmax(yt)\n",
    "            ncorr += 1\n",
    "        end\n",
    "    end\n",
    "    println(100 * (ncorr/ntot))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Chain(Dense(28^2, dim2, relu), Flux.normalise, Dense(dim2, 10), softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model(x_train[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "function base_train(epochs=100)\n",
    "    baseopt = Flux.ADAM(0.0001); \n",
    "    for i in 1:epochs\n",
    "        rid = rand(1:60000)\n",
    "        x = float(x_train[rid,:])\n",
    "        #x = reshape(x,(1,784))\n",
    "        yt = y_train[rid,:]\n",
    "        ps = Flux.params(base_model)\n",
    "        gs = gradient(ps) do\n",
    "            ŷ = base_model(x)\n",
    "            lossfn(ŷ,yt)\n",
    "        end\n",
    "        Flux.update!(baseopt,ps,gs)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@time begin\n",
    "    base_train(25000)\n",
    "end\n",
    "#25k iter: 111 seconds to 95% acc, max acc is 97.5 (if train to convergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function base_test_acc(xs,ys)\n",
    "    ncorr = 0\n",
    "    ntot = size(ys)[1]\n",
    "    for i in 1:ntot\n",
    "        #rid = rand(1:size(xs)[1])\n",
    "        x = float(xs[i,:])\n",
    "        #x = reshape(x,(1,784))\n",
    "        yt = ys[i,:]\n",
    "        ŷ = vec(base_model(x));\n",
    "        if argmax(ŷ) == argmax(yt)\n",
    "            ncorr += 1\n",
    "        end\n",
    "    end\n",
    "    println(100 * (ncorr/ntot))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test_acc(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ecd5b0def40d11926eb769baa0f5c5b6e342746dd5b6c5b03c4754dea9d1700"
  },
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
